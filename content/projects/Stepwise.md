---
date: '2024-07-15'
title: 'Stepwise DPO Fine-Tuning with TinyLlama'
github: 'https://github.com/FRAGGERR/Reward-Model-DPO'
external: ''
tech:
  - DPOTrainer
  - TinyLlama-1.1B
  - Reward Modeling
  - CausalLM
company: ''
showInProjects: true
---

Implemented a stepwise **Direct Preference Optimization (DPO)** pipeline using the TinyLlama-1.1B-Chat model.

<!-- Built a custom reward model for fine-tuning and evaluation, enabling more interpretable reasoning improvements.
Replicated OpenAI’s *“Let’s Verify Step by Step”* methodology, showcasing traceability, experimental rigor, and achieving enhanced reasoning performance on small-scale LLMs. -->
